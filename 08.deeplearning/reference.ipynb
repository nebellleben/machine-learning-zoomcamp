{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdrPf8R5T3Zm"
      },
      "source": [
        "# Deep Learning Homework - Reference Solution\n",
        "\n",
        "This notebook contains a complete implementation for the deep learning homework assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r-yunUSiT3Zn"
      },
      "outputs": [],
      "source": [
        "# Apply the suggested random number generation process set by the question for reproduceability sake:\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipUXXJRKT3Zo"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n",
        "\n",
        "Model structure:\n",
        "* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n",
        "* Next, create a convolutional layer (`nn.Conv2d`):\n",
        "    * Use 32 filters (output channels)\n",
        "    * Kernel size should be `(3, 3)` (that's the size of the filter)\n",
        "    * Use `'relu'` as activation\n",
        "* Reduce the size of the feature map with max pooling (`nn.MaxPool2d`)\n",
        "    * Set the pooling size to `(2, 2)`\n",
        "* Turn the multi-dimensional result into vectors using `flatten` or `view`\n",
        "* Next, add a `nn.Linear` layer with 64 neurons and `'relu'` activation\n",
        "* Finally, create the `nn.Linear` layer with 1 neuron - this will be the output\n",
        "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
        "\n",
        "As optimizer use `torch.optim.SGD` with the following parameters:\n",
        "* `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XcNSMugTT3Zo"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Try to import torchsummary, but it's optional\n",
        "try:\n",
        "    from torchsummary import summary\n",
        "    HAS_TORCHSUMMARY = True\n",
        "except ImportError:\n",
        "    HAS_TORCHSUMMARY = False\n",
        "    print(\"torchsummary not available. Install with: pip install torchsummary\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jTuX5F7VT3Zp"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model\n",
        "class BinaryCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layer: (3, 200, 200) -> (32, 198, 198)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3))\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Max pooling: (32, 198, 198) -> (32, 99, 99)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        # Flatten: (32, 99, 99) -> (32 * 99 * 99,)\n",
        "        # First linear layer: 32 * 99 * 99 -> 64\n",
        "        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Output layer: 64 -> 1 (binary classification)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution + ReLU\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        # Max pooling\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # First fully connected layer + ReLU\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        # Output layer + Sigmoid\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jd76qO7vT3Zp"
      },
      "outputs": [],
      "source": [
        "# Create model instance\n",
        "model = BinaryCNN()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
        "\n",
        "# Loss function for binary classification\n",
        "criterion = nn.BCELoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uVBcUgET3Zp"
      },
      "source": [
        "## Question 1: Which loss function you will use?\n",
        "\n",
        "For binary classification with sigmoid activation, we use **BCELoss** (Binary Cross Entropy Loss).\n",
        "\n",
        "Answer: **nn.BCELoss()** (or **nn.BCEWithLogitsLoss()** if we didn't use sigmoid)\n",
        "\n",
        "Since we're using sigmoid activation in the output layer, we use `nn.BCELoss()`.\n",
        "If we didn't use sigmoid, we would use `nn.BCEWithLogitsLoss()` which combines sigmoid and BCE loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGj8W_O1T3Zp",
        "outputId": "9e1ac760-00d4-4976-dc20-1d574eec4210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error using torchsummary: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
            "Counting manually...\n",
            "\n",
            "Total parameters: 20,073,473\n",
            "Trainable parameters: 20,073,473\n"
          ]
        }
      ],
      "source": [
        "# Question 2: Total number of parameters\n",
        "# Method 1: Using torchsummary (if available)\n",
        "if HAS_TORCHSUMMARY:\n",
        "    try:\n",
        "        summary(model, (3, 200, 200))\n",
        "    except Exception as e:\n",
        "        print(f\"Error using torchsummary: {e}\")\n",
        "        print(\"Counting manually...\")\n",
        "else:\n",
        "    print(\"torchsummary not available, counting manually...\")\n",
        "\n",
        "# Method 2: Count manually\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Manual calculation breakdown:\n",
        "# Conv2d: 3 * 32 * 3 * 3 + 32 (bias) = 896\n",
        "# Linear1: 32 * 99 * 99 * 64 + 64 (bias) = 20,074,048\n",
        "# Linear2: 64 * 1 + 1 (bias) = 65\n",
        "# Total: 896 + 20,074,048 + 65 = 20,075,009\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28TnsG9VT3Zp"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We'll use the **straight/curly hair** dataset from\n",
        "`http://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip`.\n",
        "\n",
        "Once unzipped, the dataset has the following folder structure:\n",
        "- `data/train/`\n",
        "- `data/test/`\n",
        "\n",
        "Each split contains two subfolders (e.g. `curly/` and `straight/`) with images.\n",
        "We'll:\n",
        "- Download and unzip the dataset (if not already present)\n",
        "- Resize images to 200x200 as required\n",
        "- Use `ImageFolder` to load images\n",
        "- Keep it as a **binary** problem (curly vs straight).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0wyufwzT3Zp",
        "outputId": "6223f2d1-3ca0-4d32-90f1-584a85ea0207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Unzipping dataset...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Download and unzip straight/curly hair dataset if not already present\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "DATA_URL = \"http://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\"\n",
        "DATA_ZIP_PATH = \"./data.zip\"\n",
        "DATA_DIR = \"./data\"\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(DATA_URL, DATA_ZIP_PATH)\n",
        "    print(\"Unzipping dataset...\")\n",
        "    with zipfile.ZipFile(DATA_ZIP_PATH, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"Done!\")\n",
        "else:\n",
        "    print(\"Dataset already present, skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqg2rKA-T3Zp",
        "outputId": "e5499a9b-329f-4918-95c0-515289c9c8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['curly', 'straight']\n",
            "Training samples: 800\n",
            "Test samples: 201\n"
          ]
        }
      ],
      "source": [
        "# Path to straight/curly hair dataset (after unzipping data.zip)\n",
        "data_dir = \"./data\"\n",
        "\n",
        "# Define transforms for training (without augmentation initially)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load dataset splits using ImageFolder\n",
        "# Expected structure:\n",
        "# ./data/train/curly\n",
        "# ./data/train/straight\n",
        "# ./data/test/curly\n",
        "# ./data/test/straight\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "test_dir = os.path.join(data_dir, \"test\")\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
        "test_dataset = ImageFolder(test_dir, transform=transform_test)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Since this dataset is already binary (curly vs straight),\n",
        "# we don't need to remap labels; ImageFolder will give 0/1 labels.\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QW5y0qcnT3Zq"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.float().to(device).unsqueeze(1)  # Convert to float and add dimension\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.float().to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvg_LAVPT3Zq",
        "outputId": "2e21d686-67e0-4f54-a65a-3e4768eceeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_GejdntT3Zq",
        "outputId": "de894ac2-b3a9-4cc1-c723-e53bc11137fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.73it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6961, Train Acc: 0.5212\n",
            "Test Loss: 0.6829, Test Acc: 0.6020\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:07<00:00,  3.21it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6697, Train Acc: 0.6225\n",
            "Test Loss: 0.6629, Test Acc: 0.6219\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:07<00:00,  3.20it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6376, Train Acc: 0.6525\n",
            "Test Loss: 0.6384, Test Acc: 0.6418\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:07<00:00,  3.36it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6091, Train Acc: 0.6737\n",
            "Test Loss: 0.6305, Test Acc: 0.6269\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:07<00:00,  3.50it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5936, Train Acc: 0.6763\n",
            "Test Loss: 0.6369, Test Acc: 0.6617\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:08<00:00,  3.06it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5718, Train Acc: 0.7025\n",
            "Test Loss: 0.6575, Test Acc: 0.6368\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:08<00:00,  2.94it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5955, Train Acc: 0.6800\n",
            "Test Loss: 0.6148, Test Acc: 0.6368\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:08<00:00,  3.06it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5633, Train Acc: 0.7050\n",
            "Test Loss: 0.6335, Test Acc: 0.6567\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:07<00:00,  3.37it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5401, Train Acc: 0.7375\n",
            "Test Loss: 0.6294, Test Acc: 0.6617\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:07<00:00,  3.24it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5348, Train Acc: 0.7250\n",
            "Test Loss: 0.6229, Test Acc: 0.6517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train the model for 10 epochs (initial training)\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNT83FMvT3Zq"
      },
      "source": [
        "## Question 3: Median of training accuracy for all epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7xRDxNRT3Zq",
        "outputId": "0065895d-a7a1-4aa4-dbac-ff4486592ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median training accuracy: 0.6781\n",
            "\n",
            "Training accuracies: ['0.5212', '0.6225', '0.6525', '0.6737', '0.6763', '0.7025', '0.6800', '0.7050', '0.7375', '0.7250']\n",
            "\n",
            "Answer: 0.68\n"
          ]
        }
      ],
      "source": [
        "# Question 3: Median of training accuracy\n",
        "median_train_acc = np.median(train_accuracies)\n",
        "print(f\"Median training accuracy: {median_train_acc:.4f}\")\n",
        "print(f\"\\nTraining accuracies: {[f'{acc:.4f}' for acc in train_accuracies]}\")\n",
        "print(f\"\\nAnswer: {median_train_acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfDIszL0T3Zq"
      },
      "source": [
        "## Question 4: Standard deviation of training loss for all epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s_PfbD_T3Zq",
        "outputId": "bef941aa-9bd6-46a2-f804-bcd77bd6bfb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard deviation of training loss: 0.0506\n",
            "\n",
            "Training losses: ['0.6961', '0.6697', '0.6376', '0.6091', '0.5936', '0.5718', '0.5955', '0.5633', '0.5401', '0.5348']\n",
            "\n",
            "Answer: 0.051\n"
          ]
        }
      ],
      "source": [
        "# Question 4: Standard deviation of training loss\n",
        "std_train_loss = np.std(train_losses)\n",
        "print(f\"Standard deviation of training loss: {std_train_loss:.4f}\")\n",
        "print(f\"\\nTraining losses: {[f'{loss:.4f}' for loss in train_losses]}\")\n",
        "print(f\"\\nAnswer: {std_train_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzjf2iI_T3Zq"
      },
      "source": [
        "## Questions 5 & 6: Training with Data Augmentation\n",
        "\n",
        "Now we'll train for 10 more epochs with data augmentation. Note: we continue training the same model, not creating a new one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhuYwNCRT3Zq",
        "outputId": "c799d37d-193a-49b1-a94b-5ac66a2f5153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation transforms applied on straight/curly hair dataset:\n",
            "- Random horizontal flip (p=0.5)\n",
            "- Random rotation (10 degrees)\n",
            "- Color jitter (brightness=0.2, contrast=0.2)\n"
          ]
        }
      ],
      "source": [
        "# Define transforms with augmentation for training\n",
        "transform_train_aug = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Reload training dataset with augmentation from straight/curly hair data\n",
        "train_dataset_aug = ImageFolder(train_dir, transform=transform_train_aug)\n",
        "\n",
        "# No need to convert labels; still binary 0/1\n",
        "\n",
        "# Create new data loader with augmentation\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Data augmentation transforms applied on straight/curly hair dataset:\")\n",
        "print(\"- Random horizontal flip (p=0.5)\")\n",
        "print(\"- Random rotation (10 degrees)\")\n",
        "print(\"- Color jitter (brightness=0.2, contrast=0.2)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtzf13wlT3Zr",
        "outputId": "30facef7-74f7-4b9e-9d74-31f6d89847a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.70it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5732, Train Acc: 0.7137\n",
            "Test Loss: 0.6139, Test Acc: 0.6617\n",
            "\n",
            "Epoch 2/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.76it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5664, Train Acc: 0.6900\n",
            "Test Loss: 0.6105, Test Acc: 0.6219\n",
            "\n",
            "Epoch 3/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.72it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5657, Train Acc: 0.7013\n",
            "Test Loss: 0.6203, Test Acc: 0.6567\n",
            "\n",
            "Epoch 4/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.74it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5458, Train Acc: 0.7212\n",
            "Test Loss: 0.6327, Test Acc: 0.6716\n",
            "\n",
            "Epoch 5/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:08<00:00,  2.90it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:02<00:00,  3.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5568, Train Acc: 0.7150\n",
            "Test Loss: 0.6119, Test Acc: 0.6816\n",
            "\n",
            "Epoch 6/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:08<00:00,  2.92it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5547, Train Acc: 0.7050\n",
            "Test Loss: 0.6223, Test Acc: 0.6766\n",
            "\n",
            "Epoch 7/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.75it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5600, Train Acc: 0.7113\n",
            "Test Loss: 0.6214, Test Acc: 0.6766\n",
            "\n",
            "Epoch 8/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.73it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5343, Train Acc: 0.7300\n",
            "Test Loss: 0.5955, Test Acc: 0.6965\n",
            "\n",
            "Epoch 9/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.73it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5490, Train Acc: 0.7188\n",
            "Test Loss: 0.6254, Test Acc: 0.6766\n",
            "\n",
            "Epoch 10/10 (with augmentation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 25/25 [00:09<00:00,  2.73it/s]\n",
            "Evaluating: 100%|██████████| 7/7 [00:01<00:00,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5358, Train Acc: 0.7188\n",
            "Test Loss: 0.6130, Test Acc: 0.6915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Train for 10 more epochs with augmentation\n",
        "# Note: We're continuing to train the same model, not creating a new one\n",
        "num_epochs_aug = 10\n",
        "train_losses_aug = []\n",
        "train_accuracies_aug = []\n",
        "test_losses_aug = []\n",
        "test_accuracies_aug = []\n",
        "\n",
        "for epoch in range(num_epochs_aug):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs_aug} (with augmentation)\")\n",
        "\n",
        "    # Train with augmented data\n",
        "    train_loss, train_acc = train_epoch(model, train_loader_aug, criterion, optimizer, device)\n",
        "    train_losses_aug.append(train_loss)\n",
        "    train_accuracies_aug.append(train_acc)\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses_aug.append(test_loss)\n",
        "    test_accuracies_aug.append(test_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgxAL8PiT3Zr"
      },
      "source": [
        "## Question 5: Mean of test loss for all epochs (with augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQGnxYjCT3Zr",
        "outputId": "72fbb89d-1599-49d1-94c1-af1695df5208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean test loss (with augmentation): 0.6167\n",
            "\n",
            "Test losses: ['0.6139', '0.6105', '0.6203', '0.6327', '0.6119', '0.6223', '0.6214', '0.5955', '0.6254', '0.6130']\n",
            "\n",
            "Answer: 0.617\n"
          ]
        }
      ],
      "source": [
        "# Question 5: Mean of test loss for all epochs with augmentation\n",
        "mean_test_loss_aug = np.mean(test_losses_aug)\n",
        "print(f\"Mean test loss (with augmentation): {mean_test_loss_aug:.4f}\")\n",
        "print(f\"\\nTest losses: {[f'{loss:.4f}' for loss in test_losses_aug]}\")\n",
        "print(f\"\\nAnswer: {mean_test_loss_aug:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJIMvacT3Zr"
      },
      "source": [
        "## Question 6: Average of test accuracy for the last 5 epochs (epochs 6-10) with augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLODtWGsT3Zr",
        "outputId": "9966a810-7ce6-4fec-d2b2-53a90f5e5a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracies for epochs 6-10: ['0.6766', '0.6766', '0.6965', '0.6766', '0.6915']\n",
            "Average test accuracy (last 5 epochs): 0.6836\n",
            "\n",
            "Answer: 0.68\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Average of test accuracy for last 5 epochs (epochs 6-10, indices 5-9)\n",
        "last_5_test_acc = test_accuracies_aug[5:10]  # Epochs 6-10 (0-indexed: 5-9)\n",
        "avg_last_5_test_acc = np.mean(last_5_test_acc)\n",
        "\n",
        "print(f\"Test accuracies for epochs 6-10: {[f'{acc:.4f}' for acc in last_5_test_acc]}\")\n",
        "print(f\"Average test accuracy (last 5 epochs): {avg_last_5_test_acc:.4f}\")\n",
        "print(f\"\\nAnswer: {avg_last_5_test_acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP-ER09eT3Zr"
      },
      "source": [
        "## Summary of Answers\n",
        "\n",
        "1. **Question 1**: Loss function - `nn.BCELoss()` (or `nn.BCEWithLogitsLoss()` if no sigmoid)\n",
        "2. **Question 2**: Total parameters - ~20,075,009\n",
        "3. **Question 3**: Median training accuracy - Check output above\n",
        "4. **Question 4**: Standard deviation of training loss - Check output above\n",
        "5. **Question 5**: Mean test loss (with augmentation) - Check output above\n",
        "6. **Question 6**: Average test accuracy (last 5 epochs with augmentation) - Check output above\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}