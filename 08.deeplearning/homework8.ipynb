{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM9hzVnKAQd03Y6FU2ICa3L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":81,"metadata":{"id":"Pcc8nDweXiZD","executionInfo":{"status":"ok","timestamp":1764607017908,"user_tz":-480,"elapsed":18,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"outputs":[],"source":["# Apply the suggested random number generation process set by the question for reproduceability sake:\n","\n","import numpy as np\n","import torch\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"XcNSMugTT3Zo","executionInfo":{"status":"ok","timestamp":1764607017910,"user_tz":-480,"elapsed":1,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"outputs":[],"source":["# Import necessary libraries\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import ImageFolder\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image\n","\n","# Try to import torchsummary, but it's optional\n","try:\n","    from torchsummary import summary\n","    HAS_TORCHSUMMARY = True\n","except ImportError:\n","    HAS_TORCHSUMMARY = False\n","    print(\"torchsummary not available. Install with: pip install torchsummary\")\n","\n"]},{"cell_type":"markdown","source":["## Model Architecture\n","\n","For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n","\n","Model structure:\n","* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n","* Next, create a convolutional layer (`nn.Conv2d`):\n","    * Use 32 filters (output channels)\n","    * Kernel size should be `(3, 3)` (that's the size of the filter)\n","    * Use `'relu'` as activation\n","* Reduce the size of the feature map with max pooling (`nn.MaxPool2d`)\n","    * Set the pooling size to `(2, 2)`\n","* Turn the multi-dimensional result into vectors using `flatten` or `view`\n","* Next, add a `nn.Linear` layer with 64 neurons and `'relu'` activation\n","* Finally, create the `nn.Linear` layer with 1 neuron - this will be the output\n","    * The output layer should have an activation - use the appropriate activation for the binary classification case\n","\n","As optimizer use `torch.optim.SGD` with the following parameters:\n","* `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)`\n"],"metadata":{"id":"PGdNRirzXuBo"}},{"cell_type":"code","execution_count":82,"metadata":{"id":"jTuX5F7VT3Zp","executionInfo":{"status":"ok","timestamp":1764607018587,"user_tz":-480,"elapsed":16,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"outputs":[],"source":[]},{"cell_type":"code","source":["# Define HairDataset\n","\n","class HairDataset(Dataset):\n","    def __init__(self, data_dir, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.image_paths = []\n","        self.labels = []\n","        self.classes = sorted(os.listdir(data_dir))\n","        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n","\n","        for label_name in self.classes:\n","            label_dir = os.path.join(data_dir, label_name)\n","            for img_name in os.listdir(label_dir):\n","                self.image_paths.append(os.path.join(label_dir, img_name))\n","                self.labels.append(self.class_to_idx[label_name])\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"],"metadata":{"id":"z6qmq3rJlq7v","executionInfo":{"status":"ok","timestamp":1764607018589,"user_tz":-480,"elapsed":15,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":["## Question 1: Which loss function you will use?\n","\n","For binary classification with sigmoid activation, we use **BCELoss** (Binary Cross Entropy Loss).\n","\n","Answer: **nn.BCELoss()** (or **nn.BCEWithLogitsLoss()** if we didn't use sigmoid)\n"],"metadata":{"id":"tAkW_Ag3YDn7"}},{"cell_type":"code","execution_count":84,"metadata":{"id":"Jd76qO7vT3Zp","executionInfo":{"status":"ok","timestamp":1764607018591,"user_tz":-480,"elapsed":14,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"outputs":[],"source":["# Loss function for binary classification\n","criterion = nn.BCELoss()\n"]},{"cell_type":"markdown","source":["## Question 2: Total number of parameters\n"],"metadata":{"id":"m6S2xfRQYLab"}},{"cell_type":"code","source":["# Question 2: Total number of parameters\n","\n","# Use Method 2: Count manually\n","total_params = sum(p.numel() for p in model.parameters())\n","\n","print(f\"\\nTotal parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3vUxjQWYDGE","executionInfo":{"status":"ok","timestamp":1764607018591,"user_tz":-480,"elapsed":12,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"3fac6ace-bd6a-4ddd-c7c9-01ed261bfab0"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Total parameters: 20,073,473\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cAmVj_TlbDPp","executionInfo":{"status":"ok","timestamp":1764607018592,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28TnsG9VT3Zp"},"source":["## Data Preparation\n","\n","We'll use the **straight/curly hair** dataset from\n","`http://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip`.\n","\n","Once unzipped, the dataset has the following folder structure:\n","- `data/train/`\n","- `data/test/`\n","\n","Each split contains two subfolders (e.g. `curly/` and `straight/`) with images.\n","We'll:\n","- Download and unzip the dataset (if not already present)\n","- Resize images to 200x200 as required\n","- Keep it as a **binary** problem (curly vs straight).\n"]},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0wyufwzT3Zp","executionInfo":{"status":"ok","timestamp":1764607018597,"user_tz":-480,"elapsed":7,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"e95d6787-5ece-4533-a63b-bdf383b8d4b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset already present, skipping download.\n"]}],"source":["# Download and unzip straight/curly hair dataset if not already present\n","import urllib.request\n","import zipfile\n","\n","DATA_URL = \"http://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\"\n","DATA_ZIP_PATH = \"./data.zip\"\n","DATA_DIR = \"./data\"\n","\n","if not os.path.exists(DATA_DIR):\n","    print(\"Downloading dataset...\")\n","    urllib.request.urlretrieve(DATA_URL, DATA_ZIP_PATH)\n","    print(\"Unzipping dataset...\")\n","    with zipfile.ZipFile(DATA_ZIP_PATH, \"r\") as zip_ref:\n","        zip_ref.extractall(\".\")\n","    print(\"Done!\")\n","else:\n","    print(\"Dataset already present, skipping download.\")"]},{"cell_type":"markdown","source":["## Generators and Training\n","- We don't need to do any additional pre-processing for the images.\n","- Use batch_size=20\n","- Use shuffle=True for both training, but False for test.\n","\n","Now fit the model."],"metadata":{"id":"-pfOCPi0aZCn"}},{"cell_type":"code","execution_count":87,"metadata":{"id":"Cqg2rKA-T3Zp","executionInfo":{"status":"ok","timestamp":1764607018598,"user_tz":-480,"elapsed":1,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"outputs":[],"source":["# @title\n","# Path to straight/curly hair dataset (after unzipping data.zip)\n","data_dir = \"./data\"\n","\n","# Define transforms for training (without augmentation initially)\n","input_size = 200\n","\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","# Simple transforms - just resize and normalize\n","train_transforms = transforms.Compose([\n","    transforms.Resize((200, 200)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=mean,\n","        std=std\n","    ) # ImageNet normalization\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize((input_size, input_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=mean, std=std)\n","])\n","\n","# Load dataset splits using ImageFolder\n","# Expected structure:\n","# ./data/train/curly\n","# ./data/train/straight\n","# ./data/test/curly\n","# ./data/test/straight\n","train_dir = os.path.join(data_dir, \"train\")\n","test_dir = os.path.join(data_dir, \"test\")\n","\n","#train_dataset = ImageFolder(train_dir, transform=transform_train)\n","#test_dataset = ImageFolder(test_dir, transform=transform_test)\n","\n","#print(f\"Classes: {train_dataset.classes}\")\n","\n","# Since this dataset is already binary (curly vs straight),\n","# we don't need to remap labels; ImageFolder will give 0/1 labels.\n","\n","# Create data loaders\n","#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","#test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","#print(f\"Training samples: {len(train_dataset)}\")\n","#print(f\"Test samples: {len(test_dataset)}\")\n"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataset = HairDataset(\n","    data_dir='./data/train',\n","    transform=train_transforms\n",")\n","\n","test_dataset = HairDataset(\n","    data_dir='./data/test',\n","    transform=test_transforms\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"],"metadata":{"id":"scAY6EOsjwkq","executionInfo":{"status":"ok","timestamp":1764607018600,"user_tz":-480,"elapsed":1,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["# Define the CNN model\n","class HairClassifierMobileNet(nn.Module):\n","    def __init__(self, num_classes=1):\n","        super(HairClassifierMobileNet, self).__init__()\n","\n","        # Convolutional layer: (3, 200, 200) -> (32, 198, 198)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3))\n","        self.relu1 = nn.ReLU()\n","\n","        # Max pooling: (32, 198, 198) -> (32, 99, 99)\n","        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n","\n","        # Flatten: (32, 99, 99) -> (32 * 99 * 99,)\n","        # First linear layer: 32 * 99 * 99 -> 64\n","        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n","        self.relu2 = nn.ReLU()\n","\n","        # Output layer: 64 -> 1 (binary classification)\n","        self.fc2 = nn.Linear(64, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Convolution + ReLU\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","\n","        # Max pooling\n","        x = self.pool(x)\n","\n","        # Flatten\n","        x = x.view(x.size(0), -1)\n","\n","        # First fully connected layer + ReLU\n","        x = self.fc1(x)\n","        x = self.relu2(x)\n","\n","        # Output layer + Sigmoid\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","\n","        return x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"id":"xlBQiTnSoYyk","executionInfo":{"status":"ok","timestamp":1764607018607,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"2b9469ec-77c8-441c-afdd-b8ed0e0f1985"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" # old version\\nclass HairClassifierMobileNet(nn.Module):\\n    def __init__(self, num_classes=2):\\n        super(HairClassifierMobileNet, self).__init__()\\n\\n        # Load pre-trained MobileNetV2\\n        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V1')\\n\\n        # Freeze base model parameters\\n        for param in self.base_model.parameters():\\n            param.requires_grad = False\\n\\n        # Remove original classifier\\n        self.base_model.classifier = nn.Identity()\\n\\n        # Add custom layers\\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d((1, 1))\\n        self.output_layer = nn.Linear(1280, num_classes)\\n\\n    def forward(self, x):\\n        x = self.base_model.features(x)\\n        x = self.global_avg_pooling(x)\\n        x = torch.flatten(x, 1)\\n        x = self.output_layer(x)\\n        return x\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create model instance\n","model = HairClassifierMobileNet(num_classes=1)\n","model.to(device);\n","\n","# Optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n"],"metadata":{"id":"dfqFtZo7n-4N","executionInfo":{"status":"ok","timestamp":1764607018609,"user_tz":-480,"elapsed":0,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# Fitting model\n","\n","def fit(model):\n","  num_epochs = 10\n","  history = {'acc': [], 'loss': [], 'test_acc': [], 'test_loss': []}\n","\n","  for epoch in range(num_epochs):\n","      model.train()\n","      running_loss = 0.0\n","      correct_train = 0\n","      total_train = 0\n","      for images, labels in train_loader:\n","          images, labels = images.to(device), labels.to(device)\n","          labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n","\n","          optimizer.zero_grad()\n","          outputs = model(images)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          running_loss += loss.item() * images.size(0)\n","          # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n","          predicted = (torch.sigmoid(outputs) > 0.5).float()\n","          total_train += labels.size(0)\n","          correct_train += (predicted == labels).sum().item()\n","\n","      epoch_loss = running_loss / len(train_dataset)\n","      epoch_acc = correct_train / total_train\n","      history['loss'].append(epoch_loss)\n","      history['acc'].append(epoch_acc)\n","\n","      model.eval()\n","      test_running_loss = 0.0\n","      correct_test = 0\n","      total_test = 0\n","      with torch.no_grad():\n","          for images, labels in test_loader:\n","              images, labels = images.to(device), labels.to(device)\n","              labels = labels.float().unsqueeze(1)\n","\n","              outputs = model(images)\n","              loss = criterion(outputs, labels)\n","\n","              test_running_loss += loss.item() * images.size(0)\n","              predicted = (torch.sigmoid(outputs) > 0.5).float()\n","              total_test += labels.size(0)\n","              correct_test += (predicted == labels).sum().item()\n","\n","      test_epoch_loss = test_running_loss / len(test_dataset)\n","      test_epoch_acc = correct_test / total_test\n","      history['test_loss'].append(test_epoch_loss)\n","      history['test_acc'].append(test_epoch_acc)\n","\n","      print(f\"Epoch {epoch+1}/{num_epochs}, \"\n","            f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n","            f\"Test Loss: {test_epoch_loss:.4f}, Test Acc: {test_epoch_acc:.4f}\")\n","\n","  return history\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJDVmaPKaheN","executionInfo":{"status":"ok","timestamp":1764607108738,"user_tz":-480,"elapsed":90128,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"ca62a76c-6af1-4f14-eb9f-d4c8ce1ac168"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.6500, Acc: 0.4869, Test Loss: 0.5966, Test Acc: 0.4876\n","Epoch 2/10, Loss: 0.5670, Acc: 0.4869, Test Loss: 0.6158, Test Acc: 0.4876\n","Epoch 3/10, Loss: 0.5171, Acc: 0.4869, Test Loss: 0.5932, Test Acc: 0.4876\n","Epoch 4/10, Loss: 0.4743, Acc: 0.4869, Test Loss: 0.6195, Test Acc: 0.4876\n","Epoch 5/10, Loss: 0.4134, Acc: 0.4869, Test Loss: 0.7173, Test Acc: 0.4876\n","Epoch 6/10, Loss: 0.4359, Acc: 0.4869, Test Loss: 0.6727, Test Acc: 0.4876\n","Epoch 7/10, Loss: 0.3327, Acc: 0.4869, Test Loss: 0.7522, Test Acc: 0.4876\n","Epoch 8/10, Loss: 0.2913, Acc: 0.4869, Test Loss: 1.1109, Test Acc: 0.4876\n","Epoch 9/10, Loss: 0.2779, Acc: 0.4869, Test Loss: 0.6870, Test Acc: 0.4876\n","Epoch 10/10, Loss: 0.4130, Acc: 0.4869, Test Loss: 0.6621, Test Acc: 0.4876\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wd0zqFWZyPpT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KNT83FMvT3Zq"},"source":["## Question 3: Median of training accuracy for all epochs\n"]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7xRDxNRT3Zq","executionInfo":{"status":"ok","timestamp":1764607269934,"user_tz":-480,"elapsed":12,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"795a0e5c-0b97-4556-c7a9-03c6897952ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Median training accuracy: 0.4869\n","\n","Training accuracies: ['0.4869', '0.4869', '0.4869', '0.4869', '0.4869', '0.4869', '0.4869', '0.4869', '0.4869', '0.4869']\n","\n","Answer: 0.49\n"]}],"source":["# Question 3: Median of training accuracy\n","median_train_acc = np.median(history['acc'])\n","print(f\"Median training accuracy: {median_train_acc:.4f}\")\n","print(f\"\\nTraining accuracies: {[f'{acc:.4f}' for acc in history['acc']]}\")\n","print(f\"\\nAnswer: {median_train_acc:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"wfDIszL0T3Zq"},"source":["## Question 4: Standard deviation of training loss for all epochs\n"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-s_PfbD_T3Zq","executionInfo":{"status":"ok","timestamp":1764607300857,"user_tz":-480,"elapsed":11,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"020257e7-5a2d-47e5-dfcb-db0480a4d17a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Standard deviation of training loss: 0.1134\n","\n","Training losses: ['0.6500', '0.5670', '0.5171', '0.4743', '0.4134', '0.4359', '0.3327', '0.2913', '0.2779', '0.4130']\n","\n","Answer: 0.113\n"]}],"source":["# Question 4: Standard deviation of training loss\n","std_train_loss = np.std(history['loss'])\n","print(f\"Standard deviation of training loss: {std_train_loss:.4f}\")\n","print(f\"\\nTraining losses: {[f'{loss:.4f}' for loss in history['loss']]}\")\n","print(f\"\\nAnswer: {std_train_loss:.3f}\")\n"]},{"cell_type":"markdown","source":["## Data Augmentation\n","\n","Add the augmentations to your training data generator"],"metadata":{"id":"c9TLCCA6yzv7"}},{"cell_type":"code","source":["transform_train_aug = transforms.Compose([\n","    transforms.Resize((200, 200)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=mean,\n","        std=std\n","    ),\n","    transforms.RandomRotation(50),\n","    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n","    transforms.RandomHorizontalFlip()\n","])\n","\n","transform_test_aug = transforms.Compose([\n","    transforms.Resize((input_size, input_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=mean, std=std),\n","    transforms.RandomRotation(50),\n","    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n","    transforms.RandomHorizontalFlip()\n","])\n","\n","\n","# Reload training dataset with augmentation from straight/curly hair data\n","train_dataset_aug = HairDataset(\n","    data_dir='./data/train',\n","    transform=transform_train_aug\n",")\n","\n","test_dataset_aug = HairDataset(\n","    data_dir='./data/test',\n","    transform=transform_test_aug\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)"],"metadata":{"id":"DfKydVIky7gw","executionInfo":{"status":"ok","timestamp":1764607859050,"user_tz":-480,"elapsed":619,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["# Train for 10 more epochs with augmentation\n","# Note: We're continuing to train the same model, not creating a new one\n","num_epochs_aug = 10\n","train_losses_aug = []\n","train_accuracies_aug = []\n","test_losses_aug = []\n","test_accuracies_aug = []\n","\n","for epoch in range(num_epochs_aug):\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs_aug} (with augmentation)\")\n","\n","    # Train with augmented data\n","    train_loss, train_acc = train_epoch(model, train_loader_aug, criterion, optimizer, device)\n","    train_losses_aug.append(train_loss)\n","    train_accuracies_aug.append(train_acc)\n","\n","    # Evaluate\n","    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n","    test_losses_aug.append(test_loss)\n","    test_accuracies_aug.append(test_acc)\n","\n","    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n","    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"2GGpnoU7y_E7","executionInfo":{"status":"error","timestamp":1764607885293,"user_tz":-480,"elapsed":12,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"297f1298-de5a-433b-9115-ac3c018b9ab2"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/10 (with augmentation)\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'train_epoch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-296584466.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train with augmented data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_losses_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_accuracies_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_epoch' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"KgxAL8PiT3Zr"},"source":["## Question 5: Mean of test loss for all epochs (with augmentation)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQGnxYjCT3Zr","executionInfo":{"status":"ok","timestamp":1764599722910,"user_tz":-480,"elapsed":8,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"72fbb89d-1599-49d1-94c1-af1695df5208"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean test loss (with augmentation): 0.6167\n","\n","Test losses: ['0.6139', '0.6105', '0.6203', '0.6327', '0.6119', '0.6223', '0.6214', '0.5955', '0.6254', '0.6130']\n","\n","Answer: 0.617\n"]}],"source":["# Question 5: Mean of test loss for all epochs with augmentation\n","mean_test_loss_aug = np.mean(test_losses_aug)\n","print(f\"Mean test loss (with augmentation): {mean_test_loss_aug:.4f}\")\n","print(f\"\\nTest losses: {[f'{loss:.4f}' for loss in test_losses_aug]}\")\n","print(f\"\\nAnswer: {mean_test_loss_aug:.3f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"aFJIMvacT3Zr"},"source":["## Question 6: Average of test accuracy for the last 5 epochs (epochs 6-10) with augmentation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLODtWGsT3Zr","executionInfo":{"status":"ok","timestamp":1764599722921,"user_tz":-480,"elapsed":2,"user":{"displayName":"Kelvin Chan","userId":"06209833744663348285"}},"outputId":"9966a810-7ce6-4fec-d2b2-53a90f5e5a14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracies for epochs 6-10: ['0.6766', '0.6766', '0.6965', '0.6766', '0.6915']\n","Average test accuracy (last 5 epochs): 0.6836\n","\n","Answer: 0.68\n"]}],"source":["# Question 6: Average of test accuracy for last 5 epochs (epochs 6-10, indices 5-9)\n","last_5_test_acc = test_accuracies_aug[5:10]  # Epochs 6-10 (0-indexed: 5-9)\n","avg_last_5_test_acc = np.mean(last_5_test_acc)\n","\n","print(f\"Test accuracies for epochs 6-10: {[f'{acc:.4f}' for acc in last_5_test_acc]}\")\n","print(f\"Average test accuracy (last 5 epochs): {avg_last_5_test_acc:.4f}\")\n","print(f\"\\nAnswer: {avg_last_5_test_acc:.2f}\")\n"]}]}